{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natanielj/T4GSpring25/blob/main/Gemini_Workflow_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68L6ByHYnUTU"
      },
      "outputs": [],
      "source": [
        "import config\n",
        "from IPython.display import clear_output\n",
        "!pip install -q -U google-generativeai # import the Gemini API\n",
        "from google.colab import userdata # import the secret key\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWQTrb4ovXix"
      },
      "source": [
        "# Google Gemini API\n",
        "Google's Gemini is a Large Language Model (LLM) that works similarly to ChatGPT. However, as we are using Gemini-Pro (free tier of Gemini), it is not as powerful as GPT-4 (paid), so you may need to generate responses a few times to achieve your desired output. This is also a good test to see how well-defined your prompt is!\n",
        "\n",
        "The Gemini API allows you to interact with Google's Gemini LLM through your program.\n",
        "\n",
        "Here we create a function that can easily call and receive a response from the Gemini API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BlBxKCLn0EA"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# Defining an API_call function, here we are using gemini-pro as it is free!\n",
        "def API_call(model, GOOGLE_API_KEY, prompt):\n",
        "    \"\"\"\n",
        "    Creates a call to the API\n",
        "    Takes in a model to choose which API\n",
        "    Returns the updated tokens (int, int) and API response (str)\n",
        "    \"\"\"\n",
        "    if model[0:6] == \"gemini\":\n",
        "        genai.configure(api_key=GOOGLE_API_KEY)\n",
        "        model = genai.GenerativeModel(model)\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "\n",
        "    else: raise (f\"Error, model {model} not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITzKyMF0ofDa"
      },
      "outputs": [],
      "source": [
        "# You can set up your Google Gemini API Key here: https://aistudio.google.com/app/apikey\n",
        "# Add this key to the secrets tab (looks like a key) on the left toolbar on Google Colab. Make you sure you enable Notebook Access!\n",
        "GOOGLE_API_KEY = userdata.get(config.api_key)\n",
        "\n",
        "# Not Recommended: you could also paste your key directly here but it isn't as secure: GOOGLE_API_KEY = your_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLN6xFRtvkYA"
      },
      "source": [
        "# Creating the Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biQ3EwW3tokp"
      },
      "outputs": [],
      "source": [
        "# You can add your prompt here:\n",
        "prompt = f''' This is my prompt for running with Gemini\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF1CWv65rdGO"
      },
      "outputs": [],
      "source": [
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnJVQyoHvqtL"
      },
      "source": [
        "# Calling the Gemini API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4VoLX9usRYx"
      },
      "outputs": [],
      "source": [
        "# Here we actually call the Gemini API. We are using gemini-pro as it is free! Gemini then provides you with a response\n",
        "response = API_call(\"gemini-pro\", GOOGLE_API_KEY, prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmQBK7T_snaY"
      },
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mdk3Luigwi8J"
      },
      "source": [
        "At this point you have Gemini's response and could choose to use string parsing to print / store specific parts of it.\n",
        "\n",
        "If you do choose to use another model like GPT, the format should be pretty similar but you will likely have to change some parts of the API_call function (mainly response variable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5I2V0SkLuOS"
      },
      "source": [
        "# Prompt 2\n",
        "You can use the response from the previous prompt output as an input in the next prompt as below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9P51mYCIIFL8"
      },
      "outputs": [],
      "source": [
        "# prompt 2 uses the \"response\" from prompt 1 as an input\n",
        "prompt_2 = f\" By putting response like this: {response} we can insert it in our second prompt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsMaQuGdLSFD"
      },
      "outputs": [],
      "source": [
        "response_2 = API_call(\"gemini-pro\", GOOGLE_API_KEY, prompt_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrN8wkq9LVwb",
        "outputId": "1898b5e3-ff98-4754-ec26-ad9d6e10c5b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| Relevant Themes | Excerpts |\n",
            "|---|---|\n",
            "| Parental Involvement: Potential Career Facilitators | Excerpt 1: \"Our parents have a lot of careers they can share with? So definitely, that's an avenue, sharing their stories, supporting their kids and getting to career experiences, job shadowing, and internships, providing those things as well, for students in the schools.\" |\n",
            "| Underutilized Potential of Parental Involvement in Career Exploration | Excerpt 2: \"Those are always pair partner. And we haven't harnessed that really well. I'll be honest, we tried to engage our parents around a lot of things, but I haven't done a lot of outreach about \"do you want to come share your career at school?\" |\n",
            "| Strategies for Enhancing Parental Involvement: Marketing and Outreach | Excerpt 3: \"I would probably say that your future is our business could do more marketing to get people to come share their careers through us. Because we have such a big audience, that we could send their information out to our families to find out who might want to become a career speaker, who might want to offer an internship.\" |\n"
          ]
        }
      ],
      "source": [
        "print(response_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpyNEVH6MhRH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
